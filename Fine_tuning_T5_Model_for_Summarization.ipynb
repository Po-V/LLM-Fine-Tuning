{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K5WXoozgZc1"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install rouge-score nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "model_checkpoint = 'google-t5/t5-small'"
      ],
      "metadata": {
        "id": "-oPBFfZ0g9uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from evaluate import load\n",
        "\n",
        "raw_dataset = load_dataset('big_patent', 'd')\n",
        "eval_metric = load('rouge')"
      ],
      "metadata": {
        "id": "kg_o43zVg9xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset"
      ],
      "metadata": {
        "id": "ntBLsFPMg90f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset['train']['description'][:1]"
      ],
      "metadata": {
        "id": "m6eHz5OyyyUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "IrojkTtCg93Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(['This is sentence 1', 'this is sentence 2'])"
      ],
      "metadata": {
        "id": "NPdJk3-NHvcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "tokenized_train_test_source = concatenate_datasets([raw_dataset['train'], raw_dataset['test']]).map(lambda x : tokenizer(raw_dataset['train'], truncation = True), batched = True, remove_columns = ['description', 'abstract'])\n",
        "max_source_length = max([len(x) for x in tokenized_train_test_source['input_ids']])\n",
        "print(max_source_length)\n",
        "\n",
        "tokenized_train_test_target = concatenate_datasets([raw_dataset['train'], raw_dataset['test']]).map(lambda x : tokenizer(raw_dataset['train'], truncation = True), batched = True, remove_columns =['description', 'abstract'])\n",
        "max_target_length = max([len(x) for x in tokenized_train_test_target['input_ids']])\n",
        "print(max_target_length)"
      ],
      "metadata": {
        "id": "g81DfGtKg96K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(samples):\n",
        "  inputs = [\"summarize: \" + text word text in samples['description']]\n",
        "  model_inputs = tokenizer(inputs, max_length = max_source_length, truncation = True)\n",
        "\n",
        "  model_targets = tokenizer(text_target= samples['abstract'], max_length = max_target_length, truncation = True)\n",
        "\n",
        "  model_inputs['labels'] = model_targets['input_ids']\n",
        "\n",
        "  return model_inputs\n",
        "\n",
        "tokenized_dataset = raw_dataset.map(preprocess_function, batched = True)"
      ],
      "metadata": {
        "id": "FejorAGRg987"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "CQtNhQttg9_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model = model)"
      ],
      "metadata": {
        "id": "ghRuIwGVg-Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir = f\"{model_checkpoint.split('/')[1]}-big_patent-d\",\n",
        "    per_device_train_batch_size = 8,\n",
        "    per_device_eval_batch_size = 8,\n",
        "    predict_with_generate = True,\n",
        "    fp16 = True,\n",
        "    learning_rate = 3e-5,\n",
        "    num_train_epochs = 3,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    push_to_hub = False,\n",
        "    save_total_limit = 3\n",
        ")"
      ],
      "metadata": {
        "id": "DRblkOa5g-FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens = True)\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens = True)\n",
        "\n",
        "  predictions = [pred.strip() for pred in decoded_preds]\n",
        "  labels = [label.strip() for label in decoded_labels]\n",
        "  predictions = ['\\n'.join(sent_tokenize(pred)) for pred in predictions]\n",
        "  labels = ['\\n'.join(sent_tokenize(label)) for label in labels]\n",
        "  result = eval_metric.compute(predictions=predictions, references=labels, use_stemmer=True, use_aggregator=True)\n",
        "\n",
        "  result = {k: round(v * 100, 4) for k, v in result.items()}\n",
        "  prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "  result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "P1P-RkB6rodb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = tokenized_dataset['train'],\n",
        "    eval_dataset = tokenized_dataset['test'],\n",
        "    compute_metrics = compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "TMeiTId3g-IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "gKkcyBAbg-LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xRLy62KQzy_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bqM40dOLg-Rr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}